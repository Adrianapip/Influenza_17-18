{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Question 2: What strains and subtypes were most common in the US in 2017-18?\n",
    "\n",
    "Answering questions 2 and 3 will require retrieving records from sequence databases.\n",
    "\n",
    "#### Data source\n",
    "\n",
    "The datasets in this project are from the [Influenza Research Database](https://www.fludb.org/brc/home.spg?decorator=influenza), a viral data repository that pulls from [NCBI GenBank](https://www.ncbi.nlm.nih.gov/genbank/) and [NCBI RefSeq](https://www.ncbi.nlm.nih.gov/refseq/).\n",
    "\n",
    "I'm going to pull records according to strain, for both flus A and B, and for flu season 17-18.\n",
    "\n",
    "** Note: This analysis is performed only on data from the Influenza Research Database and is not a complete analysis of the 2017-18 flu season as not all US flu cases may have been reported, sequenced and deposited in this database. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Preparing the Data: Moving flu raw data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data analysis libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper', font_scale=1.)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'raw_data/fluA_strains.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b0039c6d08a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the flu datasets and look at the available column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_fluA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_data/fluA_strains.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_fluB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_data/fluB_strains.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'raw_data/fluA_strains.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "# Load the flu datasets and look at the available column names\n",
    "\n",
    "df_fluA = pd.read_csv('fluA_strains.tsv', sep='\\t')\n",
    "df_fluB = pd.read_csv('fluB_strains.tsv', sep='\\t')\n",
    "\n",
    "df_fluA.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of columns to work with, including subtypes to known mutations or drug resistance. I'm going to keep it simple in the beginning before moving onto sequence data and just explore some demographics such as:\n",
    "* subtypes; \n",
    "* locations; and \n",
    "* collection dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first few rows of data for Flu A\n",
    "df_fluA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is the first few rows for Flu B\n",
    "df_fluB.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Extracting Subtype Data by Month \n",
    "\n",
    "Now that the DataFrames have been loaded, I can view how many records exist and what information they hold. This will help in determining what tables/figures may be appropriate to neatly summariza the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluA.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 5k records, and I'd like to limit this investigation to strains collected from Humans. Let's take a look to see if there are any other \"Host\" animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluA['Host'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plenty of bird hosts along with the unfortunate dog and pig, so let's make a new database for just human hosts named:\n",
    "\n",
    "df_fluA_Human and df_fluB_Human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluA_Human = df_fluA[df_fluA['Host'] == 'Human']\n",
    "df_fluB_Human = df_fluB[df_fluB['Host'] == 'Human']\n",
    "df_fluA_Human.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a proper DataFrame containing records from Human infections, let's look at for just Flu A:\n",
    "\n",
    "##### General Info\n",
    "* How many records: 'n_records' \n",
    "* What were the top 5 states reporting: 'top5_states'\n",
    "* What states had lowest reporting: 'bottom5_states'\n",
    "\n",
    "##### Subtype Details\n",
    "* How many unique subtypes are there: 'n_uni_subtypes'\n",
    "* What are those subtypes: 'uni_subtypes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_records = len(df_fluA_Human)\n",
    "top5_states = df_fluA_Human['State/Province'].value_counts().head(5)\n",
    "bottom5_states = df_fluA_Human['State/Province'].value_counts().tail(5)\n",
    "n_uni_subtypes = df_fluA_Human['Subtype'].nunique()\n",
    "uni_subtypes = df_fluA_Human['Subtype'].unique()\n",
    "\n",
    "print(\"\\033[1m\" + \"General Info\" + \"\\033[0m\")\n",
    "print(\"Total number of flu A records: {}\".format(n_records))\n",
    "print(\"Top 5 states reporting Flu A:\\n{}\\n\".format(top5_states))\n",
    "print(\"Bottom 5 states reporting Flu A:\\n{}\\n\".format(bottom5_states))\n",
    "print(\"\\033[1m\" + \"Subtype Details\" + \"\\033[0m\")\n",
    "print(\"Number of unique subtypes reported: {}\".format(n_uni_subtypes))\n",
    "print(\"Those subtypes are: {}\".format(uni_subtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Flu A, there are only four reported subtypes, but for simplicity I will count 'H1' with 'H1N1', and 'H3' with 'H3N2'. \n",
    "\n",
    "To see the yearly trend of reporting dates, let's plot out when these entries were submitted ('Collection Date) by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces \"H1\" and \"H3\" subtypes into full names\n",
    "\n",
    "df_fluA_Human = df_fluA_Human.replace(to_replace=\"H1\", value=\"H1N1\")\n",
    "df_fluA_Human = df_fluA_Human.replace(to_replace=\"H3\", value=\"H3N2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the data type of the 'Collection Date' column\n",
    "\n",
    "type(df_fluA_Human.loc[:,'Collection Date'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Collection Date is a str, convert the column to datetime objects for ease in plotting \n",
    "\n",
    "df_fluA_Human = df_fluA_Human.copy()  \n",
    "df_fluA_Human['Collection Date'] = pd.to_datetime(df_fluA_Human['Collection Date'])\n",
    "\n",
    "# check the conversion\n",
    "type(df_fluA_Human['Collection Date'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new column for month of each record. This will be the x-axis of my graph\n",
    "\n",
    "df_fluA_Human['Month'] = df_fluA_Human['Collection Date'].apply(lambda time: time.month)\n",
    "df_fluA_Human.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the 'Month' column from a random sample\n",
    "\n",
    "df_fluA_Human['Month'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I have the 'Month', I'd like to change the int to month names\n",
    "import calendar\n",
    "\n",
    "df_fluA_Human['Month'] = df_fluA_Human['Month'].apply(lambda x: calendar.month_abbr[x])\n",
    "df_fluA_Human['Month'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the records by month, starting with the beginning of the flu season in Oct 2017\n",
    "\n",
    "order = ['Oct', 'Nov', 'Dec','Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "\n",
    "sns.countplot(x='Month', data=df_fluA_Human, hue='Subtype', order = order, palette='rainbow').set_title('Reported Flu A Subtypes in US during 2017-18')\n",
    "\n",
    "# relocate the legend\n",
    "plt.legend(bbox_to_anchor = (0.8, 0.98), loc =2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "plt.savefig('FluA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial answer to Question 2: For Flu A, the only reported subtypes are H1N1 and H3N2 with H3N2 being reported at higher rates. There is a peak of cases in Dec 2017 for H3N2 and Jan 2018 for H1N1.\n",
    "\n",
    "Unlike the 2 subtypes, there are many more strains reported:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluA_Human[\"Strain Name\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are so many strains, a figure won't work, but we can list them to get an idea the most common submissions. This may prove useful later when we need to look at exact sequences.\n",
    "\n",
    "A little primer on [how to read strain names](https://www.cdc.gov/flu/about/viruses/types.htm):\n",
    "\n",
    "Antigenic type/host origin/geographical origin/strain #/isolation year (if fluA, hemagglutinin and neuraminidase description, e.g., H1N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_H1N1_strains = df_fluA_Human[df_fluA_Human[\"Subtype\"] == \"H1N1\"]\n",
    "H1N1_strains = df_H1N1_strains['Strain Name'].value_counts().head(10)\n",
    "df_H3N2_strains = df_fluA_Human[df_fluA_Human[\"Subtype\"] == \"H3N2\"]\n",
    "H3N2_strains = df_H3N2_strains['Strain Name'].value_counts().head(10)\n",
    "\n",
    "print(\"\\033[1m\" + \"Most Common Strains per Subtype\" + \"\\033[0m\")\n",
    "print(\"H1N1 strains:\\n{}\\n\".format(H1N1_strains))\n",
    "print(\"H3N2 strains:\\n{}\\n\".format(H3N2_strains))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like strains are being deposited as they are newly sequenced, with no repeating strains. \n",
    "\n",
    "Now lets perform the same analysis for Influenza Type B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluB_Human.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = len(df_fluB_Human)\n",
    "top5_states = df_fluB_Human['State/Province'].value_counts().head(5)\n",
    "bottom5_states = df_fluB_Human['State/Province'].value_counts().tail(5)\n",
    "n_uni_subtypes = df_fluB_Human['Subtype'].nunique()\n",
    "uni_subtypes = df_fluB_Human['Subtype'].unique()\n",
    "\n",
    "print(\"\\033[1m\" + \"General Info\" + \"\\033[0m\")\n",
    "print(\"Total number of flu B records: {}\".format(n_records))\n",
    "print(\"Top 5 states reporting Flu B:\\n{}\\n\".format(top5_states))\n",
    "print(\"Bottom 5 states reporting Flu B:\\n{}\\n\".format(bottom5_states))\n",
    "print(\"\\033[1m\" + \"Subtype Details\" + \"\\033[0m\")\n",
    "print(\"Number of unique subtypes reported: {}\".format(n_uni_subtypes))\n",
    "print(\"Those subtypes are: {}\".format(uni_subtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that the highest record type did not fill in the state field. Also there is only one subtype reported, which is unnamed. We can still take a look at the montly reporting dates and most common strains.\n",
    "\n",
    "To simplify the graphs later on, will relabel these as just \"FluB_Sub_ukwn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluB_Human = df_fluB_Human.replace(to_replace=\"-N/A-\", value=\"FluB_Sub_unkwn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also want to take a look at the strains for fluB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluB_strains = df_fluB_Human[\"Strain Name\"].value_counts().head(10)\n",
    "\n",
    "print(\"\\033[1m\" + \"Most Common Strains for Flu B\" + \"\\033[0m\")\n",
    "print(\"FluB_Sub_unkwn:\\n{}\\n\".format(fluB_strains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also no repeat strains for fluB. Will continue to convert the DataFrame of fluB so it includes the month collection date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fluB_Human = df_fluB_Human.copy()  \n",
    "df_fluB_Human['Collection Date'] = pd.to_datetime(df_fluB_Human['Collection Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# check the conversion\n",
    "type(df_fluB_Human['Collection Date'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a new column for just the month of each record. This will be the x-axis of my graph\n",
    "\n",
    "df_fluB_Human['Month'] = df_fluB_Human['Collection Date'].apply(lambda time: time.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# The month is an int conver to --> month as str\n",
    "import calendar\n",
    "\n",
    "df_fluB_Human['Month'] = df_fluB_Human['Month'].apply(lambda x: calendar.month_abbr[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Plots the records by month, starting with the beginning of the flu season in Oct 2017\n",
    "\n",
    "order = ['Oct', 'Nov', 'Dec','Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "\n",
    "sns.countplot(x='Month', data=df_fluB_Human, hue='Subtype', order = order, palette='rainbow').set_title('Reported Flu B Subtypes in US during 2017-18')\n",
    "\n",
    "# relocate the legend\n",
    "plt.legend(bbox_to_anchor = (0.68, 0.98), loc =2, borderaxespad=0.)\n",
    "\n",
    "plt.savefig('FluB.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at just Flu B for the season, with a peak around January with about 500 submissions. \n",
    "\n",
    "Let's see how it compares to Flu A, by combining this all into one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AllFlu = pd.merge(df_fluA_Human, df_fluB_Human, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Plotting out all three subtypes\n",
    "order = ['Oct', 'Nov', 'Dec','Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "\n",
    "sns.countplot(x='Month', data=df_AllFlu, hue='Subtype', order = order, palette='rainbow').set_title('Reported Flu Subtypes in US during 2017-18')\n",
    "\n",
    "# relocate the legend\n",
    "plt.legend(bbox_to_anchor = (0.68, 0.98), loc =2, borderaxespad=0.)\n",
    "\n",
    "plt.savefig('Flu_allsubtypes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a cumulative look at all three subtypes: Flu A H1N1, Flu A H3N2, and Flu B 'N/A'. What's interesting is how Flu B persists during the end while Flu A is slowly waning out for the season.\n",
    "\n",
    "Although these figures answer Question 2, it'd be fun to take it one step further and look at these data using a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Looking at Subtypes on a US Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports libraries for a choropleth map\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we have to do is convert the state names to abbreviations so they read into the map correctly.\n",
    "\n",
    "Here's a dictionary of state names to abbreviations thanks to [rogerallen](https://gist.github.com/rogerallen/1583593)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_abbrev = {state: abbrev for abbrev, state in us_state_abbrev.items()}\n",
    "\n",
    "df_byState['abbrev'] = df_byState['State/Province'].map(us_state_abbrev)\n",
    "df_byState.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do H1N1 cases first.\n",
    "\n",
    "df_byState_H1N1 = df_byState_H1N1.copy()\n",
    "df_byState_H1N1 = df_byState[df_byState['Subtype'] == 'H1N1']\n",
    "df_byState_H1N1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data_H1N1 = dict(\n",
    "        type = 'choropleth',\n",
    "        colorscale = 'Greens',\n",
    "        reversescale = True,\n",
    "        locations = df_byState_H1N1['abbrev'],\n",
    "        z = df_byState_H1N1['counts'],\n",
    "        locationmode = 'USA-states',\n",
    "        text = ['State/Province'],\n",
    "        marker = dict(line = dict(color = 'rgb(255,255,255)',width = 1)),\n",
    "        colorbar = {'title':'reported H1N1 cases'}\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "layout = dict(title = 'Reported/Sequenced Influenza H1N1 for 2017-2018',\n",
    "              geo = dict(scope='usa',\n",
    "                         showlakes = True,\n",
    "                         lakecolor = 'rgb(85,173,240)')\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choromap = go.Figure(data = [data_H1N1],layout = layout)\n",
    "iplot(choromap,validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Here is a map for H3N2\n",
    "\n",
    "df_byState_H3N2 = df_byState[df_byState['Subtype'] == 'H3N2']\n",
    "df_byState_H3N2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data_H3N2 = dict(\n",
    "        type = 'choropleth',\n",
    "        colorscale = 'Blues',\n",
    "        reversescale = True,\n",
    "        locations = df_byState_H3N2['abbrev'],\n",
    "        z = df_byState_H3N2['counts'],\n",
    "        locationmode = 'USA-states',\n",
    "        text = ['State/Province'],\n",
    "        marker = dict(line = dict(color = 'rgb(255,255,255)',width = 1)),\n",
    "        colorbar = {'title':'reported H3N2 cases'}\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "layout = dict(title = 'Reported/Sequenced Influenza H3N2 for 2017-2018',\n",
    "              geo = dict(scope='usa',\n",
    "                         showlakes = True,\n",
    "                         lakecolor = 'rgb(85,173,240)')\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "choromap = go.Figure(data = [data_H3N2],layout = layout)\n",
    "iplot(choromap,validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there's a general idea of what this dataset holds, in my next analysis I will download sequences for analysis. To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
